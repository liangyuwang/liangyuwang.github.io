---
layout: single
title: "Projects"
permalink: /projects/
author_profile: true
---

<div class="projects-page">
  <p class="projects-intro">
    A curated list of my open-source research projects on efficient LLM training and inference systems.
  </p>

  <div class="proj-grid">
    <div class="proj-card">
      <h3 class="proj-title">
        ZO2: Full-Parameter Fine-Tuning 175B LLMs with 18GB GPU Memory
        <span class="github-stars"><a href="https://github.com/liangyuwang/zo2"><img src="https://img.shields.io/github/stars/liangyuwang/zo2?style=social" alt="ZO2 GitHub stars"></a></span>
      </h3>
      <p class="proj-desc">
        Zeroth-order offloading framework that enables memory-efficient full-parameter fine-tuning for extremely large LLMs.
      </p>
    </div>

    <div class="proj-card">
      <h3 class="proj-title">
        Train Large Model from Scratch
        <span class="github-stars"><a href="https://github.com/liangyuwang/train-large-model-from-scratch"><img src="https://img.shields.io/github/stars/liangyuwang/train-large-model-from-scratch?style=social" alt="Train from Scratch GitHub stars"></a></span>
      </h3>
      <p class="proj-desc">
        A minimal yet practical pre-training stack for GPT-style models with modular architecture and distributed training utilities.
      </p>
    </div>
  </div>

  <div class="tiny-suite-card">
    <h3 class="projects-subtitle">Tiny-LLM-Libs</h3>
    <p class="projects-intro">
      Educational mini-replicas of major distributed training stacks, designed for reading core mechanisms quickly.
    </p>

    <div class="tiny-mini-grid">
      <div class="tiny-mini-card">
        <div class="tiny-mini-head">
          <strong>Tiny-FSDP</strong>
          <span class="github-stars"><a href="https://github.com/liangyuwang/Tiny-FSDP"><img src="https://img.shields.io/github/stars/liangyuwang/Tiny-FSDP?style=social" alt="Tiny-FSDP GitHub stars"></a></span>
        </div>
        <p>DDP/ZeRO-3/FSDP side-by-side implementations for communication and memory trade-off learning.</p>
      </div>

      <div class="tiny-mini-card">
        <div class="tiny-mini-head">
          <strong>Tiny-DeepSpeed</strong>
          <span class="github-stars"><a href="https://github.com/liangyuwang/Tiny-DeepSpeed"><img src="https://img.shields.io/github/stars/liangyuwang/Tiny-DeepSpeed?style=social" alt="Tiny-DeepSpeed GitHub stars"></a></span>
        </div>
        <p>Minimal DDP + ZeRO1/2/3 training stack with meta initialization and overlap primitives.</p>
      </div>

      <div class="tiny-mini-card">
        <div class="tiny-mini-head">
          <strong>Tiny-Megatron</strong>
          <span class="github-stars"><a href="https://github.com/liangyuwang/Tiny-Megatron"><img src="https://img.shields.io/github/stars/liangyuwang/Tiny-Megatron?style=social" alt="Tiny-Megatron GitHub stars"></a></span>
        </div>
        <p>Educational TP/DP/2D hybrid pipeline with custom modules and runtime auto-tuning.</p>
      </div>
    </div>
  </div>
</div>