---
Aramcolayout: archive
title: "CV"
permalink: /cv/
author_profile: true
redirect_from:
  - /resume
---
{% include base_path %}

Education
=========

* Ph.D in [King Abdullah University of Science and Technology](https://www.kaust.edu.sa/), 2026 (expected)
* M.S. in [The Chinese University of Hong Kong](https://www.cuhk.edu.hk/), 2022
* B.S. in [Tianjin University](https://www.tju.edu.cn/english/), 2021

Work experience
===============

* July 2025 - Present: Research Intern
  * [Alibaba Qwen Team](https://qwen.ai/home)
  * Duties includes: Designed and implemented Canzona, a unified, asynchronous, and load-balanced framework to enable distributed matrix-based optimizers (e.g., Muon / Shampoo / SOAP) in large-scale LLM pretraining under Megatron with ZeRO-1 and Tensor Parallelism.

* Summer 2024: LLM Pretraining Engineer (Intern)
  * [Aramco](https://www.aramco.com/en)
  * Duties includes: Conducted large-scale LLM pretraining, optimized training efficiency through CUDA kernel fusion, asynchronous checkpointing, and distributed parallelism (ZeRO, DP, TP, PP).
  <!-- * Supervisor: [Salma Alsinan](https://www.linkedin.com/in/salma-alsinan/overlay/about-this-profile/) -->

* Fall 2022: Research Assistant
  * [King Abdullah University of Science and Technology](https://www.kaust.edu.sa/)
  * Duties included: Distributed Federated Learning
  <!-- * Supervisor: [Di Wang](https://shao3wangdi.github.io/) -->

Skills
======

* PyTorch / Libtorch: In-depth knowledge of PyTorch operators' workflow and implementation, including distributed training packages, and multi-threading / streaming programming.
* CUDA programming / Triton: Intermediate in CUDA stream and kernel programming, with a solid understanding of CUDA principles.
* DeepSpeed / Megatron: Experience using DeepSpeed and Megatron for distributed training, including manual implementation for optimization.
* Programming Languages: Python (Mainly for PyTorch), C/C++ (Mainly for Multi-thread, CUDA Programming, and LibTorch).

<!-- Publications
============

<ul>{% for post in site.publications reversed %}
    {% include archive-single-cv.html %}
  {% endfor %}</ul> -->

<!-- Teaching
========

<ul>{% for post in site.teaching reversed %}
    {% include archive-single-cv.html %}
  {% endfor %}</ul> --> -->

<!-- Talks
=====

<ul>{% for post in site.talks reversed %}
    {% include archive-single-talk-cv.html  %}
  {% endfor %}</ul>

Service and leadership
======================

* Currently signed in to 43 different slack teams -->
