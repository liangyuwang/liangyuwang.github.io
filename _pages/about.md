---
permalink: /
title: "About Me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
I am Liangyu Wang, a Ph.D. student in Computer Science at King Abdullah University of Science and Technology ([KAUST](https://www.kaust.edu.sa/en/)), specializing in efficient training and inference for large language models (LLMs) through distributed computing and advanced GPU programming. Before that, I completed my master degree at The Chinese University of Hong Kong, focusing on multimodal machine learning.

My research interests include optimizing distributed training and inference of LLMs, improving multi-threaded and multi-stream scheduling, and enhancing privacy-preserving methods for LLMs. I have interned as a LLM Pretraining Engineer at [Aramco](https://www.aramco.com/), working with large-scale GPU clusters to boost training throughput and model scalability. Currently, I am working on:

* Efficient Reinforcement Learning (RL) for LLMs Reasoning
* Distributed training and inference of LLMs
* Efficient algorithm and infrastructure design for LLMs
* Efficient privacy-preserving methods

News
====

* 04/2025: Attended the [ICLR 2025](https://openreview.net/group?id=ICLR.cc/2025/Conference), Singapore.

* 03/2025: Released ZO2 ([paper](https://arxiv.org/abs/2503.12668), [code](https://github.com/liangyuwang/zo2)).

* 08/2024: Invited to serve as a reviewer for [ICLR 2025](https://openreview.net/group?id=ICLR.cc/2025/Conference).

Projects
========

**ZO2 (Zeroth-Order Offloading): Full Parameter Fine-Tuning 175B LLMs with 18GB GPU Memory**
- A framework that enables fine-tuning of extremely large language models (like OPT-175B) on limited GPU memory through zeroth-order optimization and CPU-GPU offloading.
- Repository: [https://github.com/liangyuwang/zo2](https://github.com/liangyuwang/zo2)

Publications
============

* **ZO2: Scalable Zeroth-Order Fine-Tuning for Extremely Large Language Models with Limited GPU Memory**  
  **Liangyu Wang**, Jie Ren, Hang Xu, Junxiao Wang, Huanyi Xie, David E. Keyes, and Di Wang  
  NeurIPS workshop, 2024; arXiv preprint arXiv:2503.12668, 2025 
  [Paper](https://arxiv.org/abs/2503.12668) | [Code](https://github.com/liangyuwang/zo2)

* **FlashDP: Memory-Efficient and High-Throughput DP-SGD Training for Large Language Models**  
  **Liangyu Wang**, Junxiao Wang, Jie Ren, Zihang Xiang, David E. Keyes, and Di Wang  
  NeurIPS workshop 2024 
  [Paper](https://openreview.net/pdf?id=6izXTVVzoI)

* **WiP: Towards Light Adaptation of Large Language Models For Personal Hardware**  
  **Liangyu Wang**, Junxiao Wang and Di Wang  
  Mobisys workshop 2024 
  [Paper](https://dl.acm.org/doi/pdf/10.1145/3662006.3662065)

Contact
=======

Email: liangyu.wang@kaust.edu.sa  
GitHub: [liangyuwang](https://github.com/liangyuwang)
