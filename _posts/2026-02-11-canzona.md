---
title: "Canzona: Bringing Matrix-based Optimizers to Large-Scale Distributed Training"
date: 2026-02-11
permalink: /posts/2026/02/canzona/
tags:
  - llm systems
  - distributed training
  - optimizer
---

<div class="lang-toggle" style="display:flex; gap:0.45rem; margin:0.5rem 0 1rem;">
  <button type="button" id="lang-en-btn" style="padding:0.2rem 0.55rem; border:1px solid #cfd8ea; border-radius:999px; background:#edf3ff; color:#17479a; cursor:pointer;">English</button>
  <button type="button" id="lang-zh-btn" style="padding:0.2rem 0.55rem; border:1px solid #d6deef; border-radius:999px; background:#fff; color:#4a556f; cursor:pointer;">中文</button>
</div>

<div id="lang-en-content">

We recently released our paper **Canzona**:  
[Canzona: A Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers](https://arxiv.org/abs/2602.06079).

The goal is straightforward:  
**make matrix-based optimizers (e.g., Shampoo, Muon, SOAP) run efficiently in mainstream distributed training stacks such as Megatron.**

## What is the core challenge?

Matrix-based optimizers often require **holistic updates**, while distributed LLM training heavily relies on **tensor fragmentation** across devices.

This creates a structural mismatch:

- Synchronous solutions usually incur significant computation overhead.
- Layer-wise partitioning alone cannot resolve the mismatch while preserving communication-efficient geometry.

## Key idea of Canzona

Canzona introduces two key designs:

1. **Decoupling logical optimizer assignment from physical parameter distribution**
2. **Parallelism-aware optimization**
   - For **Data Parallelism**: alpha-Balanced Static Partitioning to mitigate load imbalance.
   - For **Tensor Parallelism**: asynchronous compute pipeline with Micro-Group Scheduling to batch fragmented updates and hide reconstruction overhead.

## Results (reported in the paper)

On Qwen3 models (up to 32B) with 256 GPUs, Canzona achieves:

- **1.57x speedup in end-to-end iteration time**
- **5.8x reduction in optimizer step latency**

## One-sentence takeaway

Canzona is less about proposing yet another optimizer, and more about making existing matrix-based optimizers truly practical in modern distributed LLM training systems.

</div>

<div id="lang-zh-content" style="display:none;">

我们最近发布了论文 **Canzona**：  
[Canzona: A Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers](https://arxiv.org/abs/2602.06079)。

这篇工作的核心目标很直接：  
**让 Shampoo、Muon、SOAP 这类矩阵优化器，在 Megatron 等主流分布式训练框架下高效落地。**

## 核心问题是什么？

矩阵优化器通常要求参数更新保持“整体性”（holistic updates），  
但大模型分布式训练会把参数切碎到不同设备（tensor fragmentation）。

这两者天然存在冲突：

- 同步式方案通常带来较高的计算冗余；
- 仅靠按层切分，难以同时满足通信效率和几何约束。

## Canzona 的关键思路

Canzona 主要做了两件事：

1. **逻辑优化器分配与物理参数分布解耦**
2. **按并行模式定制优化策略**
   - 在 **Data Parallelism** 下：提出 alpha-Balanced Static Partitioning，缓解负载不均；
   - 在 **Tensor Parallelism** 下：提出异步计算管线与 Micro-Group Scheduling，将碎片化更新批处理并隐藏重构开销。

## 论文报告的效果

在 Qwen3 系列模型（最高 32B）和 256 GPU 上，Canzona 取得了：

- **端到端 iteration time 1.57x 加速**
- **optimizer step latency 5.8x 降低**

## 一句话总结

Canzona 的重点不是再造一个新优化器，而是把现有矩阵优化器和现代分布式 LLM 训练系统更自然地对齐起来，让它们真正可用、可扩展。

</div>

<script>
  (function () {
    const enBtn = document.getElementById("lang-en-btn");
    const zhBtn = document.getElementById("lang-zh-btn");
    const en = document.getElementById("lang-en-content");
    const zh = document.getElementById("lang-zh-content");
    const storageKey = "blog_lang_pref";

    function setLang(lang) {
      const isEn = lang !== "zh";
      en.style.display = isEn ? "block" : "none";
      zh.style.display = isEn ? "none" : "block";

      enBtn.style.background = isEn ? "#edf3ff" : "#fff";
      enBtn.style.color = isEn ? "#17479a" : "#4a556f";
      enBtn.style.borderColor = isEn ? "#cfd8ea" : "#d6deef";

      zhBtn.style.background = isEn ? "#fff" : "#edf3ff";
      zhBtn.style.color = isEn ? "#4a556f" : "#17479a";
      zhBtn.style.borderColor = isEn ? "#d6deef" : "#cfd8ea";

      try {
        localStorage.setItem(storageKey, isEn ? "en" : "zh");
      } catch (e) {}
    }

    const saved = (function () {
      try {
        return localStorage.getItem(storageKey);
      } catch (e) {
        return null;
      }
    })();

    setLang(saved === "zh" ? "zh" : "en");
    enBtn.addEventListener("click", function () { setLang("en"); });
    zhBtn.addEventListener("click", function () { setLang("zh"); });
  })();
</script>

