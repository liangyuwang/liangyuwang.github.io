---
title: "Canzona: Bringing Matrix-based Optimizers to Large-Scale Distributed Training"
date: 2026-02-11
permalink: /posts/2026/02/canzona/
excerpt: "Canzona decouples logical optimizer assignment from physical parameter distribution to make matrix-based optimizers practical in large-scale distributed LLM training."
tags:
  - llm systems
  - distributed training
  - optimizer
---

<div class="lang-toggle" style="display:flex; gap:0.45rem; margin:0.5rem 0 1rem;">
  <button type="button" id="lang-en-btn" style="padding:0.2rem 0.55rem; border:1px solid #cfd8ea; border-radius:999px; background:#edf3ff; color:#17479a; cursor:pointer;">English</button>
  <button type="button" id="lang-zh-btn" style="padding:0.2rem 0.55rem; border:1px solid #d6deef; border-radius:999px; background:#fff; color:#4a556f; cursor:pointer;">中文</button>
</div>

<div id="lang-en-content">
  <p>
    We recently released our paper <strong>Canzona</strong>:
    <a href="https://arxiv.org/abs/2602.06079">Canzona: A Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers</a>.
  </p>

  <p>
    The goal is straightforward:
    <strong>make matrix-based optimizers (e.g., Shampoo, Muon, SOAP) run efficiently in mainstream distributed training stacks such as Megatron.</strong>
  </p>

  <h2>What is the core challenge?</h2>

  <p>
    Matrix-based optimizers often require <strong>holistic updates</strong>, while distributed LLM training heavily relies on
    <strong>tensor fragmentation</strong> across devices.
  </p>

  <p>This creates a structural mismatch:</p>
  <ul>
    <li>Synchronous solutions usually incur significant computation overhead.</li>
    <li>Layer-wise partitioning alone cannot resolve the mismatch while preserving communication-efficient geometry.</li>
  </ul>

  <h2>Key idea of Canzona</h2>

  <p>Canzona introduces two key designs:</p>
  <ol>
    <li><strong>Decoupling logical optimizer assignment from physical parameter distribution.</strong></li>
    <li>
      <strong>Parallelism-aware optimization.</strong>
      <ul>
        <li>For <strong>Data Parallelism</strong>: alpha-Balanced Static Partitioning to mitigate load imbalance.</li>
        <li>For <strong>Tensor Parallelism</strong>: asynchronous compute pipeline with Micro-Group Scheduling to batch fragmented updates and hide reconstruction overhead.</li>
      </ul>
    </li>
  </ol>

  <h2>Results (reported in the paper)</h2>

  <p>On Qwen3 models (up to 32B) with 256 GPUs, Canzona achieves:</p>
  <ul>
    <li><strong>1.57x speedup in end-to-end iteration time.</strong></li>
    <li><strong>5.8x reduction in optimizer step latency.</strong></li>
  </ul>

  <h2>One-sentence takeaway</h2>

  <p>
    Canzona is less about proposing yet another optimizer, and more about making existing matrix-based optimizers truly practical in modern distributed LLM training systems.
  </p>

</div>

<div id="lang-zh-content" style="display:none;">
  <p>
    我们最近发布了论文 <strong>Canzona</strong>：
    <a href="https://arxiv.org/abs/2602.06079">Canzona: A Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers</a>。
  </p>

  <p>
    这篇工作的核心目标很直接：
    <strong>让 Shampoo、Muon、SOAP 这类矩阵优化器，在 Megatron 等主流分布式训练框架下高效落地。</strong>
  </p>

  <h2>核心问题是什么？</h2>

  <p>
    矩阵优化器通常要求参数更新保持“整体性”（holistic updates），但大模型分布式训练会把参数切碎到不同设备（tensor fragmentation）。
  </p>

  <p>这两者天然存在冲突：</p>
  <ul>
    <li>同步式方案通常带来较高的计算冗余。</li>
    <li>仅靠按层切分，难以同时满足通信效率和几何约束。</li>
  </ul>

  <h2>Canzona 的关键思路</h2>

  <p>Canzona 主要做了两件事：</p>
  <ol>
    <li><strong>逻辑优化器分配与物理参数分布解耦。</strong></li>
    <li>
      <strong>按并行模式定制优化策略。</strong>
      <ul>
        <li>在 <strong>Data Parallelism</strong> 下：提出 alpha-Balanced Static Partitioning，缓解负载不均。</li>
        <li>在 <strong>Tensor Parallelism</strong> 下：提出异步计算管线与 Micro-Group Scheduling，将碎片化更新批处理并隐藏重构开销。</li>
      </ul>
    </li>
  </ol>

  <h2>论文报告的效果</h2>

  <p>在 Qwen3 系列模型（最高 32B）和 256 GPU 上，Canzona 取得了：</p>
  <ul>
    <li><strong>端到端 iteration time 1.57x 加速。</strong></li>
    <li><strong>optimizer step latency 5.8x 降低。</strong></li>
  </ul>

  <h2>一句话总结</h2>

  <p>
    Canzona 的重点不是再造一个新优化器，而是把现有矩阵优化器和现代分布式 LLM 训练系统更自然地对齐起来，让它们真正可用、可扩展。
  </p>

</div>

<script>
  (function () {
    const enBtn = document.getElementById("lang-en-btn");
    const zhBtn = document.getElementById("lang-zh-btn");
    const en = document.getElementById("lang-en-content");
    const zh = document.getElementById("lang-zh-content");
    const storageKey = "blog_lang_pref";

    function setLang(lang) {
      const isEn = lang !== "zh";
      en.style.display = isEn ? "block" : "none";
      zh.style.display = isEn ? "none" : "block";

      enBtn.style.background = isEn ? "#edf3ff" : "#fff";
      enBtn.style.color = isEn ? "#17479a" : "#4a556f";
      enBtn.style.borderColor = isEn ? "#cfd8ea" : "#d6deef";

      zhBtn.style.background = isEn ? "#fff" : "#edf3ff";
      zhBtn.style.color = isEn ? "#4a556f" : "#17479a";
      zhBtn.style.borderColor = isEn ? "#d6deef" : "#cfd8ea";

      try {
        localStorage.setItem(storageKey, isEn ? "en" : "zh");
      } catch (e) {}
    }

    const saved = (function () {
      try {
        return localStorage.getItem(storageKey);
      } catch (e) {
        return null;
      }
    })();

    setLang(saved === "zh" ? "zh" : "en");
    enBtn.addEventListener("click", function () { setLang("en"); });
    zhBtn.addEventListener("click", function () { setLang("zh"); });
  })();
</script>

