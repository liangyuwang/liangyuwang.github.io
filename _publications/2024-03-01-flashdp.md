---
title: "FlashDP: Memory-Efficient and High-Throughput DP-SGD Training for Large Language Models"
collection: publications
category: conferences
permalink: /publication/2024-03-01-flashdp
excerpt: 'This paper presents a memory-efficient and high-throughput approach for training large language models with differential privacy guarantees.'
date: 2024-03-01
venue: 'NeurIPS workshop 2024'
paperurl: 'https://openreview.net/pdf?id=6izXTVVzoI'
citation: 'Liangyu Wang, Junxiao Wang, Jie Ren, Zihang Xiang, David E. Keyes, and Di Wang. (2024). &quot;FlashDP: Memory-Efficient and High-Throughput DP-SGD Training for Large Language Models.&quot; <i>NeurIPS workshop 2024</i>.'
---

FlashDP introduces a memory-efficient and high-throughput approach for differential privacy stochastic gradient descent (DP-SGD) training of large language models. 

Our method addresses the significant memory and computational challenges of applying DP-SGD to large-scale models by optimizing the per-example gradient computation process. FlashDP makes privacy-preserving training more practical and efficient for modern LLM architectures while maintaining privacy guarantees.

[Download paper here](https://openreview.net/pdf?id=6izXTVVzoI) 