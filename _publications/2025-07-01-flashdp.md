---
title: "FlashDP: Memory-Efficient and High-Throughput DP-SGD Training for Large Language Models"
collection: publications
category: conferences
permalink: /publication/2025-07-01-flashdp
date: 2025-07-01
venue: 'NeurIPS workshop 2024'
paperurl: 'https://openreview.net/pdf?id=6izXTVVzoI'
codeurl: 'https://github.com/kaustpradalab/flashdp'
authors: 'Liangyu Wang, Junxiao Wang, Jie Ren, Zihang Xiang, David E. Keyes, and Di Wang'
abstract: 'A memory-efficient and high-throughput approach for training large language models with differential privacy guarantees.'
---

## FlashDP: Memory-Efficient and High-Throughput DP-SGD Training for Large Language Models

[**Paper**](https://openreview.net/pdf?id=6izXTVVzoI) | [![GitHub stars](https://img.shields.io/github/stars/kaustpradalab/flashdp?style=social)](https://github.com/kaustpradalab/flashdp)

<details>
<summary><strong>Abstract</strong></summary>
As large language models (LLMs) increasingly underpin technological advancements, the privacy of their training data emerges as a critical concern. Differential Privacy (DP) serves as a rigorous mechanism to protect this data, yet its integration via Differentially Private Stochastic Gradient Descent (DP-SGD) introduces substantial challenges, primarily due to the complexities of per-sample gradient clipping. Current explicit methods, such as Opacus, necessitate extensive storage for per-sample gradients, significantly inflating memory requirements. Conversely, implicit methods like GhostClip reduce storage needs by recalculating gradients multiple times, which leads to inefficiencies due to redundant computations. This paper introduces FlashDP, an innovative cache-friendly per-layer DP-SGD that consolidates necessary operations into a single task, calculating gradients only once in a fused manner. This approach not only diminishes memory movement by up to \textbf{50\%} but also cuts down redundant computations by \textbf{20\%}, compared to previous methods. Consequently, FlashDP does not increase memory demands and achieves a \textbf{90\%} throughput compared to the Non-DP method on a four-A100 system during the pre-training of the Llama-13B model, while maintaining parity with standard per-layer clipped DP-SGD in terms of accuracy. These advancements establish FlashDP as a pivotal development for efficient and privacy-preserving training of LLMs. FlashDP's code has been open-sourced in https://github.com/kaustpradalab/flashdp.
</details>

**Authors:** Liangyu Wang, Junxiao Wang, Jie Ren, Zihang Xiang, David E. Keyes, and Di Wang

**Published in:** NeurIPS workshop 2024 