---
title: "WiP: Towards Light Adaptation of Large Language Models For Personal Hardware"
collection: publications
category: conferences
permalink: /publication/2024-06-03-wip
date: 2024-06-03
venue: 'Mobisys workshop 2024'
paperurl: 'https://dl.acm.org/doi/pdf/10.1145/3662006.3662065'
authors: 'Liangyu Wang, Junxiao Wang and Di Wang'
abstract: 'Explores efficient adaptation approaches for running large language models on personal hardware with limited resources.'
---

## WiP: Towards Light Adaptation of Large Language Models For Personal Hardware

[**Paper**](https://dl.acm.org/doi/pdf/10.1145/3662006.3662065)

<details>
<summary><strong>Abstract</strong></summary>
The large language models (LLMs) that everyone is using are not deployed locally. Users need to send relatively private and important data to LLM when using it. Handing over private and important data to LLM will cause people to worry, especially now that many people have begun to use LLM to deal with life and work affairs. Such concerns cannot be easily dispelled by various guarantees and agreements. However, LLMs are often resource-intensive and computationally demanding, making the transition from server-side to device-side difficult because LLM's self-attention module contains a large number of tensor multiplications that are heavy and inefficient for hardware. While previous work proposed approximate neural operators that enable hardware-efficient implementation of multiplication-less neural networks, they introduce new challenges of significant accuracy loss, making these methods inefficient in practice. In this paper, we examine the problem of light adaptation of LLMs. We propose a new neural operator that enables the adapted LLM to obtain original accuracy without fine-tuning or only requiring a few fine-tuning steps, while our neural operator has high hardware inference efficiency.
</details>

**Authors:** Liangyu Wang, Junxiao Wang and Di Wang

**Published in:** Mobisys workshop 2024 